---
title: "Examples of statistical modeling and visualisations"
author: "by Leshun Xu"
date: "13 Mar 2019"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(randomForest, warn.conflicts = FALSE))
suppressMessages(library(XLConnect, warn.conflicts = FALSE))
suppressMessages(library(tm, warn.conflicts = FALSE))
suppressMessages(library(ggplot2, warn.conflicts = FALSE))
suppressPackageStartupMessages(library(wordcloud, warn.conflicts = FALSE))
suppressMessages(library(SnowballC, warn.conflicts = FALSE))
suppressMessages(library(RCurl, warn.conflicts = FALSE))
suppressMessages(library(XML, warn.conflicts = FALSE))
```

This R Markdown document is a demo of R interactive programme. It allows users to change assumptions of the analysis at any time, and see the results immediately.

I introduce three examples in the present file, the probability model of a distribution, the random forest model for a wild fire data set, and the cloud visualisation for frequecy words on a web page. The second example is static with two tables, while others are interactive.

```{r echo=FALSE}
# set.seed(2019)
data3 <- rnorm(500)
```


## I. Visualisation of a distribution
The following histogram is describing the distribution of the simulated data. The visualisation is interactive.

```{r eruptions, echo=FALSE}
inputPanel(
  selectInput("n_breaks", label = "Number of bars:",
              choices = c(10, 20, 50, 100), selected = 50),
  
  sliderInput("bw_adjust", label = "Density curve bandwidth:",
              min = 0.2, max = 2, value = 1, step = 0.2),

  sliderInput("q_prob", label = "Threshold quantile:",
              min = 0, max = 1, value = 0.25, step = 0.01)
)

renderPlot({
  hist(data3, probability = TRUE, breaks = as.numeric(input$n_breaks),
       col="grey",
       xlab = "Observed Data", ylab = "Density (Propotion)", main = "Distribution of the data")
  
  dens <- density(data3, adjust = input$bw_adjust)
  lines(dens, col = "blue", lwd=2)

  qua <- as.numeric(quantile(data3, as.numeric(input$q_prob)))
  points(qua, 0.0001, pch = 21, bg = "blue", cex=2.5)
})
```

## II. Wild fire prediction

I attached the modified wild fire data set which has 10,000 observations and 31 variables. By using Random Forest package, the future fire occurrence is predicted in seven ranked levels. 

An indicated sampling method is introduced to the following precedure. I randomly select 1000 observations from the data set as the sample, and randomly set 70% of the sample as the training data. Then by fitting a Random Forest model, I make a prediction for all observations. The following table is saying the accuracy of the fitted model is about 75.4%. This can be improved by a better learning method and a more informative sampling method.

```{r echo=FALSE}

alldata <- read.csv("firedata.csv")
tr4all <- alldata

tr4all[which(tr4all$FREQUENCY > 6),]$FREQUENCY <- 7
tr4all$FREQUENCY <- as.factor(tr4all$FREQUENCY)

## Leshun added on 2018-01-18 to reduce the data size.
test4all <- tr4all

set.seed(2018)
tr4all <- tr4all[sample(nrow(tr4all), 1000), ]

# Divide the original data into 70% for trainning and 30% for testing
set.seed(2013)
ind <- sample(2, nrow(tr4all), replace = TRUE, prob=c(0.7, 0.3))
set.seed(2013)
tr4all.rf70 <- randomForest(FREQUENCY ~ ., data=tr4all[ind == 1,])

rf70.pred <- predict(tr4all.rf70, test4all)
table(observed = test4all[, "FREQUENCY"], predicted = rf70.pred)
nrow(test4all[which(rf70.pred==test4all$FREQUENCY),])/nrow(test4all)
```

If I use this model to predict the sample data set, the accuracy is 93.0%.

```{r echo=FALSE}
## prediction for trainning data (70%)
rf70.pred <- predict(tr4all.rf70, tr4all)
table(observed = tr4all[, "FREQUENCY"], predicted = rf70.pred)
nrow(tr4all[which(rf70.pred==tr4all$FREQUENCY),])/nrow(tr4all)
```

## III. Text analysis
I collected the words from one Wikipedia web page (https://en.wikipedia.org/wiki/Data_science).
The following plot is an example to show the frequencies of words used on this page. Some words such as some of articles, prepositions, and conjunctions have been cleaned as the first step of data munging.

```{r echo=FALSE}
# Method 3:
url <- "https://en.wikipedia.org/wiki/Data_science"
html <- getURL(url, followlocation = TRUE)
doc <- htmlParse(html, asText=TRUE)
plain.text <- xpathSApply(doc, "//p", xmlValue)
doc_ids <- c(1)
df <- data.frame(doc_id = doc_ids, text = plain.text, stringsAsFactors = FALSE)
docs <- Corpus(DataframeSource(df))

toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})

docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(gsub), pattern = " a ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " b ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " I ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " the ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " and ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " more ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " you ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " your ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " that ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " with ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " are ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " than ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " out ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " other ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " all ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " from ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " have ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " many ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " which ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " any ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " about ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " used ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " their ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " first ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " like ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " this ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " become ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " will ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " uses ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " these ", replacement = " ")
docs <- tm_map(docs, content_transformer(gsub), pattern = " only ", replacement = " ")
docs <- tm_map(docs, stripWhitespace)

dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)

minf <- min(freq)
maxf <- max(freq)
```

```{r message=FALSE, echo=FALSE}
inputPanel(
  sliderInput("n_freq", label = "Minimum frequency for plots:",
              min = minf, max = maxf, value = 8, step = 1)
)

dtmr <-DocumentTermMatrix(docs, control=list(wordLengths=c(4, 20), bounds = list(global = c(1,27))))

freqr <- colSums(as.matrix(dtmr))

renderPlot({
  wf=data.frame(Words=names(freqr),Occurrences=freqr)
  p <- ggplot(subset(wf, freqr>=input$n_freq), aes(Words, Occurrences))
  p <- p + scale_x_discrete(name ="")
  p <- p + geom_bar(stat="identity")
  p <- p + theme(text = element_text(size=20), axis.text.x=element_text(angle=75, hjust=1))
  plot(p)
})
```

In the following cloud plot, the bigger font size implies a higher frequency of the word used on the web page.

```{r echo=FALSE}
inputPanel(
  sliderInput("n_freq2", label = "Minimum frequency for plots:",
              min = minf, max = maxf, value = 8, step = 1)
)
renderPlot(width = 768 , height = 512,{
  set.seed(20190313)
  wordcloud(names(freqr), freqr, scale = c(8, .2), min.freq=input$n_freq2, colors=brewer.pal(7,"Dark2"))
})
```


